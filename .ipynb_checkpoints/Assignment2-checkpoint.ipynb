{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cardiovascular-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "depth_images = ['./Dataset_Assignment_2/depth_0.jpg','./Dataset_Assignment_2/depth_1.jpg','./Dataset_Assignment_2/depth_2.jpg','./Dataset_Assignment_2/depth_3.jpg']\n",
    "\n",
    "images = ['./Dataset_Assignment_2/im_0.jpg','./Dataset_Assignment_2/im_1.jpg','./Dataset_Assignment_2/im_2.jpg','./Dataset_Assignment_2/im_3.jpg']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "active-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = images[0]\n",
    "d1 = depth_images[0]\n",
    "im2 = images[1]\n",
    "\n",
    "image1 = cv2.imread(im1)\n",
    "image2 = cv2.imread(im2)\n",
    "\n",
    "depth_img1 = cv2.imread(d1)\n",
    "#\n",
    "\n",
    "def get_keypoints(image):\n",
    "    sift = cv2.SIFT_create()\n",
    "    keyp,des = sift.detectAndCompute(image,None)\n",
    "    \n",
    "    return keyp,des\n",
    "\n",
    "# here we are using the Lowe's ratio test for matching\n",
    "def get_matchings(des1,des2):\n",
    "    bf_matcher = cv2.BFMatcher()\n",
    "    matchings = bf_matcher.knnMatch(des1,des2,k=2)\n",
    "    matchings_list = []\n",
    "    for a,b in matchings:\n",
    "        if(a.distance<0.7*b.distance):\n",
    "            matchings_list.append([a])\n",
    "    return matchings_list\n",
    "\n",
    "# we will now use the above functions to calculate the matches\n",
    "keyp1,des1 = get_keypoints(image1)\n",
    "keyp2,des2 = get_keypoints(image2)\n",
    "\n",
    "quality_matchings = get_matchings(des1,des2)\n",
    "\n",
    "\n",
    "# Now we are dividing the all depths interval into 5 levels\n",
    "# we will use the m = 52\n",
    "levels = [[],[],[],[],[]]\n",
    "# for inbuilt homography\n",
    "levels_query = [[],[],[],[],[]]\n",
    "levels_train = [[],[],[],[],[]]\n",
    "\n",
    "\n",
    "for i in range(len(quality_matchings)):\n",
    "    img_coordinates = list(keyp1[quality_matchings[i][0].queryIdx].pt)\n",
    "    img_coordinates2 = list(keyp2[quality_matchings[i][0].trainIdx].pt)\n",
    "    \n",
    "    x_coordinate = img_coordinates[0]\n",
    "    y_coordinate = img_coordinates[1]\n",
    "    x_coordinate1 = img_coordinates2[0]\n",
    "    y_coordinate1 = img_coordinates2[1]\n",
    "    \n",
    "    \n",
    "    depth = depth_img1[int(y_coordinate),int(x_coordinate)][0]\n",
    "    # now we have inserted a particular point in accordance with its depth\n",
    "    levels[int(np.floor(depth/52))].append([x_coordinate,y_coordinate,x_coordinate1,y_coordinate1])\n",
    "    \n",
    "    levels_query[int(np.floor(depth/52))].append(keyp1[quality_matchings[i][0].queryIdx].pt)\n",
    "    levels_train[int(np.floor(depth/52))].append(keyp2[quality_matchings[i][0].trainIdx].pt)\n",
    "    \n",
    "    \n",
    "\n",
    "# Here we are ensuring every one has got enough points else we are replacing it with its neighboour\n",
    "for i in range(len(levels)):\n",
    "    if(len(levels[i]) < 20):\n",
    "        flag = 0\n",
    "        for j in range(i+1,5):\n",
    "            if(len(levels[j])>20):\n",
    "                levels[i] = levels[j]\n",
    "                levels_query[i] = levels_query[j]\n",
    "                levels_train[i] = levels_train[j]\n",
    "                flag = 1\n",
    "                break\n",
    "        if(flag != 1):\n",
    "            for j in range(i-1,-1,-1):\n",
    "                if(len(levels[j])>20):\n",
    "                    levels[i] = levels[j]\n",
    "                    levels_query[i] = levels_query[j]\n",
    "                    levels_train[i] = levels_train[j]\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "advance-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_homography_matrix(random_points):\n",
    "    #looping through the four random points and finding the Assemble matrix\n",
    "    matra = []\n",
    "    for point in random_points:\n",
    "        point1 = np.matrix([point.item(0), point.item(1), 1])\n",
    "        point2 = np.matrix([point.item(2), point.item(3), 1])\n",
    "\n",
    "        matra2 = [0, 0, 0, -point2.item(2) * point1.item(0), -point2.item(2) * point1.item(1), -point2.item(2) * point1.item(2),\n",
    "              point2.item(1) * point1.item(0), point2.item(1) * point1.item(1), point2.item(1) * point1.item(2)]\n",
    "\n",
    "        matra1 = [-point2.item(2) * point1.item(0), -point2.item(2) * point1.item(1), -point2.item(2) * point1.item(2), 0, 0, 0,\n",
    "              point2.item(0) * point1.item(0), point2.item(0) * point1.item(1), point2.item(0) * point1.item(2)]\n",
    "        \n",
    "        matra.append(matra1)\n",
    "        matra.append(matra2)\n",
    "\n",
    "    Assemble_matrix = np.matrix(matra)\n",
    "\n",
    "    #dividing assemble matri into the svd\n",
    "    u, s, v = np.linalg.svd(Assemble_matrix)\n",
    "\n",
    "    #reshape the min singular value into a 3 by 3 matrix\n",
    "    h = np.reshape(v[8], (3, 3))\n",
    "    \n",
    "    #normalize and now we have h\n",
    "    h = (1/h.item(8)) * h\n",
    "    return h\n",
    "\n",
    "\n",
    "#### error between estimated and real #####\n",
    "def Distance_for_point(point, Homography_temp):\n",
    "\n",
    "    point1 = np.transpose(np.matrix([point[0].item(0), point[0].item(1), 1]))\n",
    "    estimate_point2 = np.dot(Homography_temp, point1)\n",
    "    estimate_point2 = (1/estimate_point2.item(2))*estimate_point2\n",
    "\n",
    "    point2 = np.transpose(np.matrix([point[0].item(2), point[0].item(3), 1]))\n",
    "    error = point2 - estimate_point2\n",
    "    return np.linalg.norm(error)\n",
    "\n",
    "\n",
    "def ransac(matches,threshold):\n",
    "    maximum_inliers = []\n",
    "    Homography_matrix = []\n",
    "    for i in range(1000):\n",
    "        #taking 4 random points for calculating homography matrix\n",
    "        a = matches[random.randrange(0, len(matches))]\n",
    "        b = matches[random.randrange(0, len(matches))]\n",
    "        \n",
    "        c = matches[random.randrange(0, len(matches))]\n",
    "        \n",
    "        d = matches[random.randrange(0, len(matches))]\n",
    "        \n",
    "        four_points = np.vstack((a, b))\n",
    "        four_points = np.vstack((four_points, c))\n",
    "        four_points = np.vstack((four_points, d))\n",
    "\n",
    "        #call the homography function on those points\n",
    "        Homography_temp = find_homography_matrix(four_points)\n",
    "        inliers = []\n",
    "\n",
    "        for i in range(len(matches)):\n",
    "            d = Distance_for_point(matches[i], Homography_temp)\n",
    "            if d < threshold:\n",
    "                inliers.append(matches[i])\n",
    "        \n",
    "\n",
    "        if len(inliers) > len(maximum_inliers):\n",
    "            maximum_inliers = inliers\n",
    "            Homography_matrix = Homography_temp\n",
    "    \n",
    "\n",
    "    return Homography_matrix, maximum_inliers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "H_inbuilt = []\n",
    "H_ours = []\n",
    "\n",
    "for i in range(len(levels)):\n",
    "    if(len(levels[i])>0):\n",
    "        abc = np.matrix(levels[i])\n",
    "        Homographymtr,inliers = ransac(abc,5)\n",
    "        \n",
    "        qr = np.matrix(levels_query[i])\n",
    "        tr = np.matrix(levels_train[i])\n",
    "       \n",
    "        Homographyinbuilt,status = cv2.findHomography(qr,tr,cv2.RANSAC)\n",
    "        H_ours.append(Homographymtr)\n",
    "        H_inbuilt.append(Homographyinbuilt)\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-quilt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
